{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Your upvoting👍 is important to me and can greatly encourage me!🥰🥰🥰**\n### **Thank YOU!**","metadata":{}},{"cell_type":"code","source":"import gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n# 📦 Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n\n# 🤐 Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\n# 📊 Define flags and variables\nis_offline = False  # Flag for online/offline mode\nis_train = True  # Flag for training mode\nis_infer = True  # Flag for inference mode\nmax_lookback = np.nan  # Maximum lookback (not specified)\nsplit_day = 435  # Split day for time series data\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":4.666556,"end_time":"2023-10-31T03:48:34.580575","exception":false,"start_time":"2023-10-31T03:48:29.914019","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:19.055648Z","iopub.execute_input":"2023-11-06T11:22:19.055903Z","iopub.status.idle":"2023-11-06T11:22:23.612566Z","shell.execute_reply.started":"2023-11-06T11:22:19.055879Z","shell.execute_reply":"2023-11-06T11:22:23.611796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 📂 Read the dataset from a CSV file using Pandas\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# 🧹 Remove rows with missing values in the \"target\" column\ndf = df.dropna(subset=[\"target\"])\n\n# 🔁 Reset the index of the DataFrame and apply the changes in place\ndf.reset_index(drop=True, inplace=True)\n\n# 📏 Get the shape of the DataFrame (number of rows and columns)\ndf_shape = df.shape\n","metadata":{"papermill":{"duration":18.0321,"end_time":"2023-10-31T03:48:52.617699","exception":false,"start_time":"2023-10-31T03:48:34.585599","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:23.614398Z","iopub.execute_input":"2023-11-06T11:22:23.614768Z","iopub.status.idle":"2023-11-06T11:22:42.246235Z","shell.execute_reply.started":"2023-11-06T11:22:23.614736Z","shell.execute_reply":"2023-11-06T11:22:42.245415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 🧹 Function to reduce memory usage of a Pandas DataFrame\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    \n    # 📏 Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # 🔄 Iterate through each column in the DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # Check if the column's data type is not 'object' (i.e., numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # Check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # ℹ️ Provide memory optimization information if 'verbose' is True\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    # 🔄 Return the DataFrame with optimized memory usage\n    return df\n","metadata":{"papermill":{"duration":0.122113,"end_time":"2023-10-31T03:48:52.755627","exception":false,"start_time":"2023-10-31T03:48:52.633514","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:42.247330Z","iopub.execute_input":"2023-11-06T11:22:42.247620Z","iopub.status.idle":"2023-11-06T11:22:42.364375Z","shell.execute_reply.started":"2023-11-06T11:22:42.247597Z","shell.execute_reply":"2023-11-06T11:22:42.363392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# 📊 Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # 🔁 Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # 🔁 Loop through rows of the DataFrame\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # 🚫 Prevent division by zero\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n","metadata":{"papermill":{"duration":0.616324,"end_time":"2023-10-31T03:48:53.387757","exception":false,"start_time":"2023-10-31T03:48:52.771433","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:42.366543Z","iopub.execute_input":"2023-11-06T11:22:42.366833Z","iopub.status.idle":"2023-11-06T11:22:42.974152Z","shell.execute_reply.started":"2023-11-06T11:22:42.366809Z","shell.execute_reply":"2023-11-06T11:22:42.973268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We use CUDF speed up pandas in `imbalance_features` func.","metadata":{}},{"cell_type":"code","source":"# 📊 Function to generate imbalance features\ndef imbalance_features(df):\n    import cudf\n    df = cudf.from_pandas(df)\n    \n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n        \n    # V2 features\n    # Calculate additional features\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    # Calculate various statistical aggregation features\n    \n        \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    # Calculate diff features for specific columns\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n    df = df.to_pandas()\n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\ndef numba_imb_features(df):\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    return df\n\n# 📅 Function to generate time and stock-related features\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    # Map global features to the DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\n# 🚀 Function to generate all features by combining imbalance and other features\ndef generate_all_features(df):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    df = numba_imb_features(df)\n    # Generate time and stock-related features\n    df = other_features(df)\n    gc.collect()  # Perform garbage collection to free up memory\n    \n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]\n","metadata":{"papermill":{"duration":0.024762,"end_time":"2023-10-31T03:48:53.427261","exception":false,"start_time":"2023-10-31T03:48:53.402499","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:42.975676Z","iopub.execute_input":"2023-11-06T11:22:42.975977Z","iopub.status.idle":"2023-11-06T11:22:42.995214Z","shell.execute_reply.started":"2023-11-06T11:22:42.975951Z","shell.execute_reply":"2023-11-06T11:22:42.994316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}","metadata":{"papermill":{"duration":0.019095,"end_time":"2023-10-31T03:48:53.451201","exception":false,"start_time":"2023-10-31T03:48:53.432106","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:42.996443Z","iopub.execute_input":"2023-11-06T11:22:42.996760Z","iopub.status.idle":"2023-11-06T11:22:43.011203Z","shell.execute_reply.started":"2023-11-06T11:22:42.996736Z","shell.execute_reply":"2023-11-06T11:22:43.010450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the code is running in offline or online mode\nif is_offline:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    \n    # Display a message indicating offline mode and the shapes of the training and validation sets\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df\n    \n    # Display a message indicating online mode\n    print(\"Online mode\")\n","metadata":{"papermill":{"duration":0.013205,"end_time":"2023-10-31T03:48:53.469231","exception":false,"start_time":"2023-10-31T03:48:53.456026","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:43.012184Z","iopub.execute_input":"2023-11-06T11:22:43.012443Z","iopub.status.idle":"2023-11-06T11:22:43.023888Z","shell.execute_reply.started":"2023-11-06T11:22:43.012414Z","shell.execute_reply":"2023-11-06T11:22:43.022823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    if is_offline:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n    else:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)\n","metadata":{"papermill":{"duration":58.653136,"end_time":"2023-10-31T03:49:52.127357","exception":false,"start_time":"2023-10-31T03:48:53.474221","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:22:43.024904Z","iopub.execute_input":"2023-11-06T11:22:43.025176Z","iopub.status.idle":"2023-11-06T11:23:35.586519Z","shell.execute_reply.started":"2023-11-06T11:22:43.025153Z","shell.execute_reply":"2023-11-06T11:23:35.585515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nimport gc\n\n# Assuming df_train_feats and df_train are already defined and df_train contains the 'date_id' column\n\n# Set up parameters for LightGBM\n\nlgb_params = {\n    \"objective\": \"mae\",\n    \"n_estimators\": 5000,\n    \"num_leaves\": 300,\n    \"subsample\": 0.6,\n    'learning_rate': 0.00871, \n    'max_depth': 11,\n    \"colsample_bytree\" : 0.8,\n    \"n_jobs\": 4,\n    \"device\": \"gpu\",\n    \"verbosity\": -1,\n    \"importance_type\": \"gain\",\n}\nfeature_name = list(df_train_feats.columns)\nprint(f\"Feature length = {len(feature_name)}\")\n\n# The total number of date_ids is 480, we split them into 5 folds with a gap of 5 days in between\nnum_folds = 5\nfold_size = 480 // num_folds\ngap = 5\n\nmodels = []\nscores = []\n\nmodel_save_path = 'modelitos_para_despues'  # Directory to save models\nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\n\n# We need to use the date_id from df_train to split the data\ndate_ids = df_train['date_id'].values\n\nfor i in range(num_folds):\n    start = i * fold_size\n    end = start + fold_size\n    \n    # Define the training and testing sets by date_id\n    if i < num_folds - 1:  # No need to purge after the last fold\n        purged_start = end - 2\n        purged_end = end + gap + 2\n        train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n    else:\n        train_indices = (date_ids >= start) & (date_ids < end)\n    \n    test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n    \n    df_fold_train = df_train_feats[train_indices]\n    df_fold_train_target = df_train['target'][train_indices]\n    df_fold_valid = df_train_feats[test_indices]\n    df_fold_valid_target = df_train['target'][test_indices]\n\n    print(f\"Fold {i+1} Model Training\")\n    \n    # Train a LightGBM model for the current fold\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_fold_train[feature_name],\n        df_fold_train_target,\n        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n\n    # Append the model to the list\n    models.append(lgb_model)\n    # Save the model to a file\n    model_filename = os.path.join(model_save_path, f'doblecito_{i+1}.txt')\n    lgb_model.booster_.save_model(model_filename)\n    print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n    # Evaluate model performance on the validation set\n    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n    scores.append(fold_score)\n    print(f\"Fold {i+1} MAE: {fold_score}\")\n\n    # Free up memory by deleting fold specific variables\n    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n    gc.collect()\n\n# Calculate the average best iteration from all regular folds\naverage_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n\n# Update the lgb_params with the average best iteration\nfinal_model_params = lgb_params.copy()\nfinal_model_params['n_estimators'] = average_best_iteration\n\nprint(f\"Training final model with average best iteration: {average_best_iteration}\")\n\n# Train the final model on the entire dataset\nfinal_model = lgb.LGBMRegressor(**final_model_params)\nfinal_model.fit(\n    df_train_feats[feature_name],\n    df_train['target'],\n    callbacks=[\n        lgb.callback.log_evaluation(period=100),\n    ],\n)\n\n# Append the final model to the list of models\nmodels.append(final_model)\n\n# Save the final model to a file\nfinal_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\nfinal_model.booster_.save_model(final_model_filename)\nprint(f\"Final model saved to {final_model_filename}\")\n\n# Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\nprint(f\"Average MAE across all folds: {np.mean(scores)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:23:35.588061Z","iopub.execute_input":"2023-11-06T11:23:35.588450Z","iopub.status.idle":"2023-11-06T11:26:06.126881Z","shell.execute_reply.started":"2023-11-06T11:23:35.588415Z","shell.execute_reply":"2023-11-06T11:26:06.126132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices) / np.sum(std_error)\n    out = prices - std_error * step\n    return out\n\nif is_infer:\n    import optiver2023\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, predictions = [], []\n    cache = pd.DataFrame()\n\n    # Weights for each fold model\n    model_weights = [1/len(models)] * len(models) \n    \n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n        if counter > 0:\n            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n        feat = generate_all_features(cache)[-len(test):]\n\n        # Generate predictions for each model and calculate the weighted average\n        lgb_predictions = np.zeros(len(test))\n        for model, weight in zip(models, model_weights):\n            lgb_predictions += weight * model.predict(feat)\n\n        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n        sample_prediction['target'] = clipped_predictions\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n\n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n","metadata":{"papermill":{"duration":52.710045,"end_time":"2023-10-31T03:55:58.288323","exception":false,"start_time":"2023-10-31T03:55:05.578278","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-06T11:26:06.129888Z","iopub.execute_input":"2023-11-06T11:26:06.130499Z","iopub.status.idle":"2023-11-06T11:27:19.492581Z","shell.execute_reply.started":"2023-11-06T11:26:06.130441Z","shell.execute_reply":"2023-11-06T11:27:19.491619Z"},"trusted":true},"execution_count":null,"outputs":[]}]}